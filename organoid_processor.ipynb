{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing organoid images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import deepcell\n",
    "import imagecodecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "from deepcell.applications import Application\n",
    "from deepcell_toolbox.processing import histogram_normalization\n",
    "from deepcell_toolbox.deep_watershed import deep_watershed as watershed_postprocessing\n",
    "\n",
    "import functools\n",
    "\n",
    "def preprocess(*args):\n",
    "    return histogram_normalization(*args, kernel_size=[32,32])\n",
    "\n",
    "def postprocess(*args):\n",
    "    return watershed_postprocessing(*args,\n",
    "                                    detection_threshold=0.25,\n",
    "                                    distance_threshold=0.1,\n",
    "                                    min_distance=2.5)\n",
    "\n",
    "model = PanopticNet('resnet50',\n",
    "                   input_shape=(256,256,1),\n",
    "                   norm_method=None,\n",
    "                   num_semantic_heads=3,\n",
    "                   num_semantic_classes=[1,1,2],\n",
    "                   location=True,\n",
    "                   include_top=True,\n",
    "                   interpolation='bilinear',\n",
    "                   lite=True)\n",
    "model.load_weights('/scratch/users/jschaepe/pfordyce/Microwells/organoid_resnet50.h5')\n",
    "\n",
    "class OrganoidSegmenter(Application):\n",
    "    def __init__(self, model=None):\n",
    "        if model is None:\n",
    "            raise ValueError('Provide a model')\n",
    "\n",
    "        super(OrganoidSegmenter, self).__init__(\n",
    "            model,\n",
    "            model_image_shape=model.input_shape[1:],\n",
    "            model_mpp=0.5,\n",
    "            preprocessing_fn=preprocess,\n",
    "            postprocessing_fn=postprocess\n",
    "        )\n",
    "    \n",
    "    def predict(self,\n",
    "                image,\n",
    "                batch_size=4,\n",
    "                image_mpp=None,\n",
    "                preprocess_kwargs={},\n",
    "                postprocess_kwargs={}):\n",
    "\n",
    "        return self._predict_segmentation(\n",
    "            image,\n",
    "            batch_size=batch_size,\n",
    "            image_mpp=image_mpp,\n",
    "            preprocess_kwargs=preprocess_kwargs,\n",
    "            postprocess_kwargs=postprocess_kwargs)\n",
    "\n",
    "OS = OrganoidSegmenter(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.exposure import equalize_adapthist, rescale_intensity\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import fnmatch\n",
    "import imageio\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# returns well info for a given experiment\n",
    "def get_well_info(metadata_file, experiment_id):\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    metadata = metadata[metadata['Experiment']==experiment_id]\n",
    "    well_dict = {'A1':1, 'A2':2, 'A3':3, 'A4':4, 'B1':5, 'B2':6, 'B3':7, 'B4':8, 'C1':9, 'C2':10, 'C3':11, 'C4':12}\n",
    "    metadata['row_number'] = metadata.apply(lambda row : well_dict[row['Well']], axis=1)\n",
    "    metadata = metadata.drop(['Experiment'], axis=1)\n",
    "    return metadata.to_numpy()\n",
    "\n",
    "# sets up an h5 file to store predictions from deepcell\n",
    "def setup_h5file(experiment_folder, experiment_id, experiment_date, well_info):\n",
    "    print('setting up h5file...')\n",
    "    title = experiment_id + '_predicted_images.h5'\n",
    "    h5file = h5py.File(title, 'w')\n",
    "    for well_id, mutant, well_number in well_info:\n",
    "        timepoints = read_timepoints(experiment_folder + well_id + '/', well_id)\n",
    "        h5group = h5file.create_group(well_id)\n",
    "        # load one image to see what size it is\n",
    "        input_folder = experiment_folder + well_id + '/'\n",
    "        mCherry_imagePath = input_folder + 'mCherry/timepoint_{0}-{1}-'.format(\n",
    "            timepoints[0], well_number) + experiment_date + '.tif'\n",
    "        imgs, n = load_images(mCherry_imagePath)\n",
    "        for i in range(n):\n",
    "            h5dset = h5group.create_dataset(name=str(i), dtype = np.uint8, shape=(len(timepoints), 4560, 4560), \n",
    "                                            chunks = True, compression = 'gzip', scaleoffset = True, shuffle = True)\n",
    "    return h5file\n",
    "\n",
    "# adds newly predicted image to h5file for storage\n",
    "def update_h5file(h5file, label_imgs_arr, well_id, j, n):\n",
    "    for i in range(n):\n",
    "        h5file[well_id][str(i)][j,:,:] = label_imgs_arr[i,:,:]\n",
    "    return h5file\n",
    "\n",
    "# loads and proprocesses images for deepcell to predict on\n",
    "def load_images(filepath):\n",
    "    imgs = io.imread(filepath)\n",
    "    n = imgs.shape[0]\n",
    "    assert(imgs.shape[1:] == (2280,2280)), 'incorrect image size: ' + imgs.shape\n",
    "    loaded_imgs = []\n",
    "    for i in range(imgs.shape[0]):\n",
    "        img = imgs[i,:,:]\n",
    "        img = np.float32(img)\n",
    "        img = skimage.transform.rescale(img, 2)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        loaded_imgs.append(img)\n",
    "    return loaded_imgs, n\n",
    "\n",
    "# reads csv with timepoint information and returns a list of strings of timepoint names\n",
    "def read_timepoints(input_folder, well_id):\n",
    "    my_file = open(input_folder + well_id + '_timepoints.txt', \"r\")\n",
    "    content = my_file.read()\n",
    "    content = content.translate({ord(c): None for c in '\\' '})\n",
    "    content = content.translate({ord(c): '_' for c in '-'})\n",
    "    timepoints = content.split(\",\")\n",
    "    return timepoints\n",
    "\n",
    "# updates the xrange and yrange to reflect the fact that the predicted image is 2x the size of the original\n",
    "def get_slices(xrange, yrange):\n",
    "    window_size = 115\n",
    "    xstart = int(2*window_size*np.floor((xrange[0]/window_size))+1)\n",
    "    xstop = xstart + window_size*2-1\n",
    "    ystart = int(2*window_size*np.floor((yrange[0]/window_size))+1)\n",
    "    ystop = ystart + window_size*2-1\n",
    "    return xstart, xstop, ystart, ystop\n",
    "\n",
    "# Adds a microwell_id for each row in the form of x_y_stack\n",
    "def microwell_id_label(row):\n",
    "    x = row['x']\n",
    "    y = row['y']\n",
    "    stack = row['stack_indexer']\n",
    "    label = str(x) + '_' + str(y) + '_' + str(stack)\n",
    "    return label\n",
    "\n",
    "# Processes one well's data over all timepoints, processes all microwells and calculates and saves statistics\n",
    "def process_well_timecourse(input_folder, experiment_date, well_id, well_number, mutant, h5file):\n",
    "    # track time\n",
    "    start = time.time()\n",
    "    prevt = start\n",
    "    timepoints = read_timepoints(input_folder, well_id)\n",
    "    for j, timepoint in enumerate(timepoints):\n",
    "        # print statements to see progress\n",
    "        print('well_id: ', well_id, ', loop: ', j, ', timepoint: ', timepoint)\n",
    "        print(input_folder + 'mCherry/timepoint_{0}-{1}-'.format(timepoint,well_number) + experiment_date + '.csv')\n",
    "        mCherry_reimport = pd.read_csv(\n",
    "            input_folder + 'mCherry/timepoint_{0}-{1}-'.format(timepoint,well_number) + experiment_date + '.csv')\n",
    "        mCherry_imagePath = input_folder + 'mCherry/timepoint_{0}-{1}-'.format(\n",
    "            timepoint, well_number) + experiment_date + '.tif'\n",
    "        print('loading image...')\n",
    "        imgs, n = load_images(mCherry_imagePath)\n",
    "        print('predicting...')\n",
    "        label_imgs = [OS.predict(img)[0,...,0] for img in imgs]\n",
    "        label_imgs_arr = np.asarray(label_imgs)\n",
    "\n",
    "        # save predicted arrays to hd5 file\n",
    "        h5file = update_h5file(h5file, label_imgs_arr, well_id, j, n)\n",
    "\n",
    "        #initialize empty storage variables\n",
    "        l = len(mCherry_reimport)\n",
    "        cell_count = np.zeros(l)\n",
    "        cell_areas = []\n",
    "        total_area = np.zeros(l)\n",
    "        centroid_x = np.zeros(l)\n",
    "        centroid_y = np.zeros(l)\n",
    "        x_slice = []\n",
    "        y_slice = []\n",
    "\n",
    "        # loop through each well\n",
    "        for i in range(len(mCherry_reimport)):\n",
    "            chamberInfo = mCherry_reimport.iloc[i]\n",
    "            x_slice.append(eval(chamberInfo.summaryImg_xslice))\n",
    "            y_slice.append(eval(chamberInfo.summaryImg_yslice))\n",
    "            # need to update slices since predicted image is 2x the size of the original\n",
    "            xstart, xstop, ystart, ystop = get_slices(x_slice[i], y_slice[i])\n",
    "            mCherry_well = label_imgs_arr[chamberInfo.stack_indexer,xstart:xstop,ystart:ystop] \n",
    "\n",
    "            # calculate statistics\n",
    "            cells = np.unique(mCherry_well[np.where(mCherry_well != 0)])\n",
    "            cell_count[i] = len(cells)\n",
    "            areas = [len(mCherry_well[np.where(mCherry_well == cell)]) for cell in cells]\n",
    "            cell_areas.append(areas)\n",
    "            total_area[i] = np.sum(areas)\n",
    "            centroid_x[i] = np.average(np.asarray(np.where(mCherry_well != 0)[1]))\n",
    "            centroid_y[i] = np.average(np.asarray(np.where(mCherry_well != 0)[0]))\n",
    "\n",
    "        # combine data into dataframe format\n",
    "        data = {'timepoint':np.full(l,timepoint), 'x':mCherry_reimport.x.to_numpy(), 'y':mCherry_reimport.y.to_numpy(), \n",
    "                'stack_indexer':mCherry_reimport.stack_indexer.to_numpy(), 'cell_count':cell_count, \n",
    "                'cell_areas':cell_areas, 'total_area':total_area, 'centroid_x':centroid_x, 'centroid_y':centroid_y, \n",
    "                'hash_str':mCherry_reimport.hash_str.to_numpy(), 'experiment_id':np.full(l, experiment_id), \n",
    "                'well_number':np.full(l,well_number), 'well_id':np.full(l, well_id), 'mutant':np.full(l, mutant), \n",
    "                'x_slice':x_slice, 'y_slice':y_slice}\n",
    "\n",
    "        # update dataframe\n",
    "        if j ==0:\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            df2 = pd.DataFrame(data)\n",
    "            df = df.append(df2, ignore_index = True)\n",
    "\n",
    "        # print and keep track of time to monitor each loop\n",
    "        currt = time.time()\n",
    "        print('loop time: ', currt - prevt)\n",
    "        prevt = currt\n",
    "    # add microwell id to every row\n",
    "    df.insert(0, 'microwell_id', df.apply(lambda row: microwell_id_label(row), axis=1).to_numpy(), False)\n",
    "    df.to_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    return h5file\n",
    "\n",
    "# Runs through each well within an experiment and processes it over the entire timecourse\n",
    "def analyze_experiment(well_info, experiment_folder, experiment_id, experiment_date, h5file):\n",
    "    print('processing experiment...')\n",
    "    # loop through each well in the experiment a process over entire timecourse\n",
    "    for well_id, mutant, well_number in well_info:  \n",
    "        input_folder = experiment_folder + well_id + '/'\n",
    "        h5file = process_well_timecourse(input_folder, experiment_date, well_id, well_number, mutant, h5file)\n",
    "    return h5file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process organoid microwell experiment\n",
    "To process a new experiment, you only need to update `experiment_id`, `experiment_folder`, `experiment_date` and `metadata_file`. Each loop over one well at one timepoint should take ~100s to process, so your total expected time will be `(# wells)*(# timepoints per well)*(# stacks per well)*11`. If this cell doesn't run all the way through, you may need to run the `h5file.close()` command in the cell below in order to run it again. You can disregard the initial warnings on the first loop.\n",
    "\n",
    "This script assumes the following file structure: <br>\n",
    "> data <br>\n",
    "> > experiment_id <br>\n",
    "> > > well_id <br>\n",
    "> > > > well_id_timepoints.txt <br>\n",
    "> > > > mCherry <br>\n",
    "> > > > > timepoint_experiment_id_timepoint-well_number-experiment_date.csv <br>\n",
    "> > > > > timepoint_experiment_id_timepoint-well_number-experiment_date.tif <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the above cell does not finish running, you may need to run the line below before trying again\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the only three lines that need to be changed to process different experiments\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "#path to folder where you saved timelapse images from timecourse processing \n",
    "experiment_folder = '/home/users/jschaepe/scratch/pfordyce/data/' + experiment_id + '/'\n",
    "# date that is at the end of each tif or csv in that experiment\n",
    "experiment_date = '20201104'\n",
    "#path to folder where you store experiment and well metadata\n",
    "metadata_file = '/home/users/jschaepe/scratch/pfordyce/Microwells/microwell_well_info.csv'\n",
    "\n",
    "well_info = get_well_info(metadata_file, experiment_id)\n",
    "# create h5file for storing predicted images\n",
    "h5file = setup_h5file(experiment_folder, experiment_id, experiment_date, well_info)\n",
    "h5file = analyze_experiment(well_info, experiment_folder, experiment_id, experiment_date, h5file)\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting specific well or microwell prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_well(experiment_id, timepoint, well_id, stack_index, experiment_folder):\n",
    "    # load in necesarry information\n",
    "    df = pd.read_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    input_folder = experiment_folder + well_id + '/'\n",
    "    timepoints = read_timepoints(input_folder, well_id)\n",
    "    timepoint_index = np.argmax(np.asarray(timepoints) == timepoint)\n",
    "    input_folder = experiment_folder + well_id + '/'\n",
    "    h5file_name = experiment_id + '_predicted_images.h5'\n",
    "    h5file = h5py.File(h5file_name, 'r')\n",
    "    well = df[df['well_id']==well_id]\n",
    "    img = h5file[well_id][str(stack_index)][timepoint_index, :, :]\n",
    "    \n",
    "    # plot microwell\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.imshow(img)\n",
    "    plt.title(experiment_id + ', ' + well_id)\n",
    "    plt.xlabel('pixels of predicted image')\n",
    "    plt.ylabel('pixels of predicted image')\n",
    "    plt.show()\n",
    "    h5file.close()\n",
    "    return\n",
    "\n",
    "# change these parameters to plot different microwells\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "experiment_folder = '/home/users/jschaepe/scratch/pfordyce/data/' + experiment_id + '/'\n",
    "well_id = 'A1'\n",
    "stack_index = 0\n",
    "first_timepoint = '20200306_185209'\n",
    "last_timepoint = '20200314_114229'\n",
    "plot_well(experiment_id, first_timepoint, well_id, stack_index, experiment_folder)\n",
    "plot_well(experiment_id, last_timepoint, well_id, stack_index, experiment_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plots a specific microwell\n",
    "def plot_microwell(experiment_id, timepoint, well_id, microwell_id, experiment_folder):\n",
    "    # load in necesarry information\n",
    "    df = pd.read_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    input_folder = experiment_folder + well_id + '/'\n",
    "    timepoints = read_timepoints(input_folder, well_id)\n",
    "    timepoint_index = np.argmax(np.asarray(timepoints) == timepoint)\n",
    "    input_folder = experiment_folder + well_id + '/'\n",
    "    h5file_name = experiment_id + '_predicted_images.h5'\n",
    "    h5file = h5py.File(h5file_name, 'r')\n",
    "    microwell = df[df['microwell_id']==microwell_id]\n",
    "    xstart, xstop, ystart, ystop = get_slices(list(eval(list(microwell['x_slice'])[0])), \n",
    "                                              list(eval(list(microwell['y_slice'])[0])))\n",
    "    img = h5file[well_id][microwell_id[-1]][timepoint_index, xstart:xstop, ystart:ystop]\n",
    "    print(len(np.where(img != 0)[0]))\n",
    "    \n",
    "    # plot microwell\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(experiment_id + ', ' + well_id + ', ' + microwell_id)\n",
    "    plt.xlabel('pixels of predicted image')\n",
    "    plt.ylabel('pixels of predicted image')\n",
    "    plt.show()\n",
    "    h5file.close()\n",
    "\n",
    "# change these parameters to plot different microwells\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "experiment_folder = '/home/users/jschaepe/scratch/pfordyce/data/' + experiment_id + '/'\n",
    "well_id = 'A1'\n",
    "# in the form of 'x_y_stackindex'\n",
    "microwell_id = '5_11_0'\n",
    "timepoint = '20200314_114229'\n",
    "plot_microwell(experiment_id, timepoint, well_id, microwell_id, experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots a specific microwell\n",
    "def plot_microwell_all_timepoints(experiment_id, well_id, microwell_id, experiment_folder):\n",
    "    # load in necesarry information\n",
    "    df = pd.read_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    input_folder = experiment_folder + well_id + '/'\n",
    "    timepoints = read_timepoints(input_folder, well_id)\n",
    "    h5file_name = experiment_id + '_predicted_images.h5'\n",
    "    h5file = h5py.File(h5file_name, 'r')\n",
    "    microwell = df[df['microwell_id']==microwell_id]\n",
    "    xstart, xstop, ystart, ystop = get_slices(list(eval(list(microwell['x_slice'])[0])), \n",
    "                                                  list(eval(list(microwell['y_slice'])[0])))\n",
    "    fig, axs = plt.subplots(int(np.ceil(len(timepoints)/10)), 10, figsize = (50, 50))\n",
    "    counterx = 0\n",
    "    countery = 0\n",
    "    for i, timepoint in enumerate(timepoints):\n",
    "        timepoint_index = np.argmax(np.asarray(timepoints) == timepoint)\n",
    "        img = h5file['A1'][microwell_id[-1]][timepoint_index, xstart:xstop, ystart:ystop]\n",
    "        axs[countery, counterx].imshow(img)\n",
    "        axs[countery, counterx].set_title(timepoint, fontsize = 16)\n",
    "        counterx += 1\n",
    "        if counterx == 10:\n",
    "            counterx = 0\n",
    "            countery += 1\n",
    "        if i == len(timepoints) - 1:\n",
    "            for j in range(int(np.ceil(len(timepoints)/10))*10 - len(timepoints)):\n",
    "                fig.delaxes(axs[countery, counterx + j])\n",
    "    \n",
    "    # plot microwell\n",
    "    fig.suptitle(experiment_id + ', ' + well_id + ', ' + microwell_id, fontsize=50, y=0.9)\n",
    "    plt.show()\n",
    "    # uncomment this if you want to save the output\n",
    "#     plt.savefig(experiment_id + '_' + well_id +'_' + microwell_id + '_all_timepoints.png')\n",
    "    h5file.close()\n",
    "\n",
    "# change these parameters to plot different microwells\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "experiment_folder = '/home/users/jschaepe/scratch/pfordyce/data/' + experiment_id + '/'\n",
    "well_id = 'A1'\n",
    "timepoint = '20200314_114229'\n",
    "# in the form of 'x_y_stackindex'\n",
    "microwell_id = '5_11_0'\n",
    "\n",
    "plot_microwell_all_timepoints(experiment_id, well_id, microwell_id, experiment_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plots\n",
    "Holoviews and Bokeh are useful for creating interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holoviews plot\n",
    "\n",
    "def plot_total_area_and_centroids(experiment_id, well_id):\n",
    "    df = pd.read_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    df_only_cells = df[df.cell_count > 0]\n",
    "    p1 = df_only_cells.hvplot.hist('total_area', groupby = 'timepoint', min_height=0, width = 400, height = 400)\n",
    "    p2 = df_only_cells.hvplot.hexbin(gridsize=10, x='centroid_x', y='centroid_y', groupby = 'timepoint', width = 500, height=400)\n",
    "    layout = p1 + p2\n",
    "\n",
    "    # save to html file\n",
    "    hv.save(layout, 'cell_count_and_location_over_timepoints.html')\n",
    "    return layout\n",
    "\n",
    "well_id = 'A1'\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "layout = plot_total_area_and_centroids(experiment_id, well_id)\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader plots\n",
    "Datashader is useful for plotting and saving large datasets quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import colorcet\n",
    "import datashader.transfer_functions as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datashader_growth_rate(experiment_id, well_id):\n",
    "    df = pd.read_csv(experiment_id + '_' + well_id + '_processed_summary.csv')\n",
    "    timepoints = df['timepoint'].unique()\n",
    "    toplot=pd.pivot_table(df,index=['timepoint'],columns=df.groupby(['timepoint']).cumcount().add(1),values=['total_area'],aggfunc='sum')\n",
    "    toplot.columns=toplot.columns.map('{0[0]}{0[1]}'.format) \n",
    "    toplot = toplot.reset_index()\n",
    "    toplot = toplot.drop('timepoint', axis=1)\n",
    "    toplot.columns = list(range(len(toplot.columns)))\n",
    "    toplot = toplot.T\n",
    "    toplot.columns = list(range(len(timepoints)))\n",
    "\n",
    "    points = len(timepoints) - 1\n",
    "    time = np.linspace(0, 1, points)\n",
    "    cvs = ds.Canvas(plot_height=400, plot_width=1000)\n",
    "    agg = cvs.line(toplot, x=time, y=list(range(points)), agg=ds.count(), axis=1)\n",
    "    img = tf.shade(agg, how='eq_hist')\n",
    "    return img\n",
    "\n",
    "# change these two to plot all growth rates in well\n",
    "well_id = 'A1'\n",
    "experiment_id = 'experiment_id' # fill this in with your experiment folder name\n",
    "\n",
    "img = datashader_growth_rate(experiment_id, well_id)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
